{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GENERAL SETUP\n",
        "We start by importing the needed libraries."
      ],
      "metadata": {
        "id": "hLluHXMj_hWR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-Xg97HdJNOg"
      },
      "source": [
        "For this we need [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) using CMake.\n",
        "Installing it might take some time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPsnG7iZFBRl",
        "outputId": "20241337-2584-4662-8fbe-f7a78e256059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.76.tar.gz (49.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.76-cp310-cp310-linux_x86_64.whl size=80963763 sha256=3f31683b3c2d3a88c6bb472feb39d8d57dab14dbb7410bba1cf41bad49210bd5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/e5/04/a5fa9e60033548f205f0db5f6ab6f59cd27bd0da7f9c51cfe7\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.76\n"
          ]
        }
      ],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Provides random tools\n",
        "import random\n",
        "# Provides json files handling\n",
        "import json\n",
        "#P rovides regular expressions\n",
        "import re\n",
        "# Allows us to go grab models from hugging face\n",
        "from huggingface_hub import hf_hub_download\n",
        "# The Llama class is a wrapper for llama cpp models\n",
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "ARNwdp-y_n_q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv6gal8sMv6L"
      },
      "source": [
        "## MODEL SETUP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxa5RKHgK1uL"
      },
      "source": [
        "This bit of code allows us to go grabe the model from Hugging Face.\n",
        "This might take some time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LXZ3uHjIFr0F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207,
          "referenced_widgets": [
            "e3513fe77d494cd2bd9a47862a875e2d",
            "e04e446739e54243876cab925b47e3c6",
            "a15ad7e46760494aa4a728823e1f1bbd",
            "17a1955e745c47a99bdc3d191756823e",
            "6c83076f584949cc9a6d20f5995e0bc9",
            "07db6f1b23f745c6b1586e711ac3ec6d",
            "ba60b427fa9a4abf98f7760050903c5b",
            "fd7078604c8a4f45b721df7f1743019e",
            "961b41a60c7c41aa96ce41d60fe3ef6e",
            "79c4ce03bc4e49609704522c54ec559e",
            "8df68635a3ba462d98affdee698128bd"
          ]
        },
        "outputId": "7f479d98-0212-4a5f-a9ec-f1d9004883cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Emollama-chat-13b-v0.1.gguf:   0%|          | 0.00/13.8G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3513fe77d494cd2bd9a47862a875e2d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "MODEL_NAME = \"myclassunil/Emollama-chat-13b-v0.1.gguf\"\n",
        "MODEL_FILE = \"Emollama-chat-13b-v0.1.gguf\"\n",
        "MODEL_PATH = hf_hub_download(MODEL_NAME,\n",
        "                             filename=MODEL_FILE,\n",
        "                             local_dir='/content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yEanZvZMZdG"
      },
      "source": [
        "Loading the model and offloading all the layers to the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h-4aAjGXGBd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52a5bd67-8bb5-42c5-e9a2-f0b54bc24eda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 21 key-value pairs and 363 tensors from /content/Emollama-chat-13b-v0.1.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = .\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 40\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type q8_0:  282 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 5120\n",
            "llm_load_print_meta: n_head           = 40\n",
            "llm_load_print_meta: n_head_kv        = 40\n",
            "llm_load_print_meta: n_layer          = 40\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
            "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 13824\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 13B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 13.02 B\n",
            "llm_load_print_meta: model size       = 12.88 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = .\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.37 MiB\n",
            "llm_load_tensors: offloading 40 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 41/41 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   166.02 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size = 13023.85 MiB\n",
            "....................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   400.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =    85.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    11.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1286\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.model': 'llama', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '2048', 'general.name': '.', 'llama.vocab_size': '32000', 'general.file_type': '7', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '40', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ],
      "source": [
        "llm = Llama(model_path=MODEL_PATH,\n",
        "            n_gpu_layers=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BASkMhLQMtk-"
      },
      "source": [
        "The model is now usable!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHUD0S8nNVdh"
      },
      "source": [
        "## DATA SETUP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NlxU4zcNdPa"
      },
      "source": [
        "Grabbing the reviews from our json file.\n",
        "If using the notebook via google colab you need to manually upload the  json review file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "wfQv1nL4NpyN"
      },
      "outputs": [],
      "source": [
        "FILENAME = 'reviews.json'\n",
        "PATH_TO_DATA = '/content/' + str(FILENAME)\n",
        "\n",
        "with open(PATH_TO_DATA, 'r', encoding='utf-8') as f:\n",
        "    sites_data = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyJYrJTbM3EF"
      },
      "source": [
        "## USING EMOLLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yp7AQTw3opS"
      },
      "source": [
        "We'll start by defining a function for each one of the five tasks accessible through EmoLLM.\n",
        "For transparency reasons, the names of the variables are taken from [EmoLLM's introductory paper](https://doi.org/10.48550/arXiv.2401.08508) (Zhiwei Liu and others, 2024)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "F06ovpdz3x6m"
      },
      "outputs": [],
      "source": [
        "# We need to define the values we'll deem acceptable from the LLM.\n",
        "\n",
        "# Possible answers for the v_oc task.\n",
        "acceptable_v_oc = [\n",
        "    '3: very positive mental state can be inferred',\n",
        "    '2: moderately positive mental state can be inferred',\n",
        "    '1: slightly positive mental state can be inferred',\n",
        "    '0: neutral or mixed mental state can be inferred',\n",
        "    '-1: slightly negative mental state can be inferred',\n",
        "    '-2: moderately negative mental state can be inferred',\n",
        "    '-3: very negative mental state can be inferred'\n",
        "    ]\n",
        "\n",
        "# Possible answers for the e_c task.\n",
        "acceptable_e_c = [\n",
        "    'anger',\n",
        "    'anticipation',\n",
        "    'disgust',\n",
        "    'fear',\n",
        "    'joy',\n",
        "    'love',\n",
        "    'optimism',\n",
        "    'pessimism',\n",
        "    'sadness',\n",
        "    'surprise',\n",
        "    'trust'\n",
        "    ]\n",
        "\n",
        "# This is the list of sentiments that are supported for ei_reg and ei_oc.\n",
        "main_sentiments = ['joy', 'anger', 'fear', 'sadness']\n",
        "\n",
        "# Possible answers for the ei_oc task.\n",
        "base_ei_oc = [\n",
        "    '0: no E can be inferred',\n",
        "    '1: low amount of E can be inferred',\n",
        "    '2: moderate amount of E can be inferred',\n",
        "    '3: high amount of E can be inferred'\n",
        "    ]\n",
        "\n",
        "# Adapting the possible answers to include particular emotions.\n",
        "acceptable_ei_oc = []\n",
        "for sentiment in main_sentiments:\n",
        "    for message in base_ei_oc:\n",
        "        acceptable_ei_oc.append(message.replace(\"E\", sentiment))\n",
        "\n",
        "\n",
        "def get_v_reg(text, max_tok):\n",
        "    \"\"\"Estimating the valence of the review and representing it as a float.\"\"\"\n",
        "    prompt = f'''\n",
        "    Human:\n",
        "    Task: Evaluate the valence intensity of the writer's mental state based on\n",
        "     the text, assigning it a real-valued score from 0 (most negative)\n",
        "     to 1 (most positive).\n",
        "    Text: {text}\n",
        "    Intensity Score:\n",
        "    '''\n",
        "    v_reg = \"Aberrant answer from EmoLLM : \"\n",
        "    raw_v_reg = llm(prompt, max_tokens=max_tok)['choices'][0]['text']\n",
        "    try:\n",
        "        if 0 <= float(raw_v_reg) <= 1:\n",
        "            v_reg = raw_v_reg\n",
        "    except ValueError:\n",
        "        print(\"Something went wrong with get_v_reg\")\n",
        "        v_reg += str(raw_v_reg)\n",
        "    return v_reg\n",
        "\n",
        "\n",
        "# For each task, the default value is \"Aberrant answer from EmoLLM\"\n",
        "# if the model does well, we replace this message with its answer\n",
        "# otherwise, we add the aberrant answer in case it holds useful data anyway.\n",
        "\n",
        "def get_v_oc(text, max_tok):\n",
        "    \"\"\"Estimates the valence of the review and represents it\n",
        "    as an ordinal class.\"\"\"\n",
        "    prompt = f'''\n",
        "        Human:\n",
        "        Task: Categorize the text into an ordinal class that best characterizes\n",
        "        the writer's mental state, considering various degrees of positive and\n",
        "        negative sentiment intensity. 3: very positive mental state can be\n",
        "        inferred. 2: moderately positive mental state can be inferred. 1:\n",
        "        slightly positive mental state can be inferred. 0: neutral or mixed\n",
        "        mental state can be inferred. -1: slightly negative mental state can\n",
        "        be inferred. -2: moderately negative mental state can be inferred. -3:\n",
        "        very negative mental state can be inferred\n",
        "        Text: {text}\n",
        "        Intensity Class:\n",
        "        '''\n",
        "    v_oc = \"Aberrant answer from EmoLLM : \"\n",
        "    raw_v_oc = llm(prompt, max_tokens=max_tok)['choices'][0]['text']\n",
        "    if raw_v_oc in acceptable_v_oc:\n",
        "        v_oc = raw_v_oc\n",
        "    else:\n",
        "        v_oc += str(raw_v_oc)\n",
        "    return raw_v_oc\n",
        "\n",
        "\n",
        "def get_e_c(text, max_tok):\n",
        "    \"\"\"Identifies which sentiments are present in the review.\"\"\"\n",
        "    prompt = f'''\n",
        "        Task: Categorize the text's emotional tone as either 'neutral or\n",
        "        no emotion' or identify the presence of one or more of the given\n",
        "        emotions (anger, anticipation, disgust, fear, joy, love, optimism,\n",
        "        pessimism, sadness, surprise, trust).\n",
        "        Text: {text}\n",
        "        This tweet contains emotions:\n",
        "        '''\n",
        "    e_c = \"Aberrant answer from EmoLLM : \"\n",
        "    raw_e_c = llm(prompt, max_tokens=max_tok)['choices'][0]['text']\n",
        "    temp_e_c = re.split(r'[^a-zA-Z]', raw_e_c)\n",
        "    while '' in temp_e_c:\n",
        "        temp_e_c.remove('')\n",
        "    is_aberrant = False\n",
        "    for sent in temp_e_c:\n",
        "        if sent not in acceptable_e_c:\n",
        "            is_aberrant = True\n",
        "            break\n",
        "    if not is_aberrant:\n",
        "        e_c = temp_e_c\n",
        "    else:\n",
        "        e_c += str(raw_e_c)\n",
        "    return e_c\n",
        "\n",
        "\n",
        "def get_ei_reg(text, max_tok, sent):\n",
        "    \"\"\"Estimates the intensity of a sentiment and represents it as a float.\"\"\"\n",
        "    prompt = f'''\n",
        "        Human:\n",
        "        Task: Assign a numerical value between 0 (least E) and 1 (most E) to\n",
        "        represent the intensity of emotion E expressed in the text.\n",
        "        Text: {text}\n",
        "        Emotion: {sent}\n",
        "        Intensity Score:\n",
        "        '''\n",
        "    ei_reg = \"Aberrant answer from EmoLLM : \"\n",
        "    raw_ei_reg = llm(prompt, max_tokens=max_tok)['choices'][0]['text']\n",
        "    try:\n",
        "        if 0 <= float(raw_ei_reg) <= 1:\n",
        "            ei_reg = raw_ei_reg\n",
        "    except ValueError:\n",
        "        print(\"Something went wrong with get_ei_reg\")\n",
        "        ei_reg += str(raw_ei_reg)\n",
        "    return ei_reg\n",
        "\n",
        "\n",
        "def get_ei_oc(text, max_tok, sent):\n",
        "    \"\"\"Estimates the intensity of a sentiment and represents it\n",
        "    as an ordinal class.\"\"\"\n",
        "    prompt = f'''\n",
        "        Task: Categorize the tweet into an intensity level of the specified\n",
        "        emotion E, representing the mental state of the tweeter. 0: no E can\n",
        "        be inferred. 1: low amount of E can be inferred. 2: moderate amount of\n",
        "        E can be inferred. 3: high amount of E can be inferred.\n",
        "        Tweet: {text}\n",
        "        Emotion: {sent}\n",
        "        Intensity Score:\n",
        "        '''\n",
        "    ei_oc = \"Aberrant answer from EmoLLM : \"\n",
        "    raw_ei_oc = llm(prompt, max_tokens=max_tok)['choices'][0]['text']\n",
        "    if raw_ei_oc in acceptable_ei_oc:\n",
        "        ei_oc = raw_ei_oc\n",
        "    else:\n",
        "        ei_oc += str(raw_ei_oc)\n",
        "    return [raw_ei_oc, ei_oc]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now create a function that applies the five tools given by EmoLLM to our reviews."
      ],
      "metadata": {
        "id": "XCu_dc6qx11u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nOPJD6ZbC67c"
      },
      "outputs": [],
      "source": [
        "def sentiment_analysis(sites, max_tok):\n",
        "    \"\"\"Processing the reviews using EmoLLM\"\"\"\n",
        "    # We create a dict to hold the sentiment analysis results.\n",
        "    sa_results = {}\n",
        "    # We create a dict for each site results.\n",
        "    for i, site in enumerate(sites):\n",
        "        sa_site = {}\n",
        "        # Processing the different reviews with the five EmoLLM tasks.\n",
        "        for j, review in enumerate(sites[site]):\n",
        "            sa_review = {\n",
        "                #'rating' : sites[site]['rating'],\n",
        "                'v_reg' : get_v_reg(review, max_tok),\n",
        "                'v_oc' : get_v_oc(review, max_tok),\n",
        "                'e_c' : get_e_c(review, max_tok),\n",
        "                'ei_reg' : [],\n",
        "                'ei_oc' : []\n",
        "            }\n",
        "            for sent in sa_review['e_c']:\n",
        "                if sent in main_sentiments:\n",
        "                    current_reg = {sent : \\\n",
        "                                   get_ei_reg(review, max_tok, sent)}\n",
        "                    current_oc = {sent : \\\n",
        "                                  get_ei_oc(review, max_tok, sent)}\n",
        "                    sa_review['ei_reg'].append(current_reg)\n",
        "                    sa_review['ei_oc'].append(current_oc)\n",
        "            sa_site[j] = sa_review\n",
        "        sa_results[i] = sa_site\n",
        "    return sa_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we have limited ressources for this project, the possibility to use GPU was limited.\n",
        "We needed to generate placeholder results.\n",
        "This function is useless for the final state of the project.\n",
        "It generates a mockup version of the output file.\n",
        "This function is not perfect. For example, regressions and ordinal classifications are not corellated.\n",
        "Furthermore, it does not generate aberrant answers."
      ],
      "metadata": {
        "id": "UEgkwXDYybVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mockup_sentiment_analysis(sites):\n",
        "    \"\"\"As GPU ressource are scarce, this function creates\n",
        "    a placeholder for data.\"\"\"\n",
        "    # We create a dict to hold the sentiment analysis results.\n",
        "    sa_results = {}\n",
        "    # We create a dict for each site results.\n",
        "    for _, site in enumerate(sites):\n",
        "        sa_site = {}\n",
        "        # Processing the different reviews with the five EmoLLM tasks.\n",
        "        for j in range(len(sites[site])):\n",
        "            sa_review = {\n",
        "            'v_reg' : str(round(random.random(), 3)),\n",
        "            'v_oc' : random.choice(acceptable_v_oc),\n",
        "            'e_c' : [],\n",
        "            'ei_reg' : [],\n",
        "            'ei_oc' : []\n",
        "            }\n",
        "\n",
        "            for _ in range(random.randrange(1, len(acceptable_e_c))):\n",
        "                random_e_c = random.choice(acceptable_e_c)\n",
        "                if random_e_c not in sa_review['e_c']:\n",
        "                    sa_review['e_c'].append(random_e_c)\n",
        "            for sent in sa_review['e_c']:\n",
        "                if sent in main_sentiments:\n",
        "                    current_reg = {sent : str(round(random.random(), 3))}\n",
        "                    random_oc = random.choice(base_ei_oc).replace(\"E\", sent)\n",
        "                    current_oc = {sent : random_oc}\n",
        "                    sa_review['ei_reg'].append(current_reg)\n",
        "                    sa_review['ei_oc'].append(current_oc)\n",
        "            sa_site[f'review_{j}'] = sa_review\n",
        "        sa_results[site] = sa_site\n",
        "    return sa_results\n"
      ],
      "metadata": {
        "id": "huJyQ89Yn296"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can dump the output of this process in a json file."
      ],
      "metadata": {
        "id": "hq9AT3Y5zB3F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "MI4YBWCa9OiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5258804b-f6e9-472d-ad3d-5972f935444f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       3.66 ms /     6 runs   (    0.61 ms per token,  1641.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =     561.09 ms /   192 tokens (    2.92 ms per token,   342.19 tokens per second)\n",
            "llama_print_timings:        eval time =     304.12 ms /     5 runs   (   60.82 ms per token,    16.44 tokens per second)\n",
            "llama_print_timings:       total time =     875.05 ms /   197 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       8.06 ms /    13 runs   (    0.62 ms per token,  1613.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =     671.89 ms /   277 tokens (    2.43 ms per token,   412.27 tokens per second)\n",
            "llama_print_timings:        eval time =     741.95 ms /    12 runs   (   61.83 ms per token,    16.17 tokens per second)\n",
            "llama_print_timings:       total time =    1433.96 ms /   289 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       4.44 ms /     6 runs   (    0.74 ms per token,  1351.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =     500.24 ms /   213 tokens (    2.35 ms per token,   425.80 tokens per second)\n",
            "llama_print_timings:        eval time =     311.80 ms /     5 runs   (   62.36 ms per token,    16.04 tokens per second)\n",
            "llama_print_timings:       total time =     823.08 ms /   218 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       4.56 ms /     6 runs   (    0.76 ms per token,  1315.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =     483.16 ms /   187 tokens (    2.58 ms per token,   387.04 tokens per second)\n",
            "llama_print_timings:        eval time =     313.14 ms /     5 runs   (   62.63 ms per token,    15.97 tokens per second)\n",
            "llama_print_timings:       total time =     807.49 ms /   192 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       3.95 ms /     5 runs   (    0.79 ms per token,  1265.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     498.56 ms /   228 tokens (    2.19 ms per token,   457.31 tokens per second)\n",
            "llama_print_timings:        eval time =     250.50 ms /     4 runs   (   62.63 ms per token,    15.97 tokens per second)\n",
            "llama_print_timings:       total time =     759.03 ms /   232 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       8.52 ms /     5 runs   (    1.70 ms per token,   586.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =     471.13 ms /   140 tokens (    3.37 ms per token,   297.16 tokens per second)\n",
            "llama_print_timings:        eval time =     245.03 ms /     4 runs   (   61.26 ms per token,    16.32 tokens per second)\n",
            "llama_print_timings:       total time =     741.71 ms /   144 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       8.74 ms /    12 runs   (    0.73 ms per token,  1373.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =     510.04 ms /   225 tokens (    2.27 ms per token,   441.14 tokens per second)\n",
            "llama_print_timings:        eval time =     691.08 ms /    11 runs   (   62.83 ms per token,    15.92 tokens per second)\n",
            "llama_print_timings:       total time =    1225.25 ms /   236 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       3.49 ms /     6 runs   (    0.58 ms per token,  1720.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =     482.53 ms /   161 tokens (    3.00 ms per token,   333.66 tokens per second)\n",
            "llama_print_timings:        eval time =     309.93 ms /     5 runs   (   61.99 ms per token,    16.13 tokens per second)\n",
            "llama_print_timings:       total time =     807.63 ms /   166 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       2.98 ms /     5 runs   (    0.60 ms per token,  1677.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =     448.97 ms /   135 tokens (    3.33 ms per token,   300.69 tokens per second)\n",
            "llama_print_timings:        eval time =     255.22 ms /     4 runs   (   63.80 ms per token,    15.67 tokens per second)\n",
            "llama_print_timings:       total time =     711.66 ms /   139 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       6.74 ms /    11 runs   (    0.61 ms per token,  1631.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =     491.25 ms /   176 tokens (    2.79 ms per token,   358.27 tokens per second)\n",
            "llama_print_timings:        eval time =     630.16 ms /    10 runs   (   63.02 ms per token,    15.87 tokens per second)\n",
            "llama_print_timings:       total time =    1137.85 ms /   186 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       3.60 ms /     6 runs   (    0.60 ms per token,  1664.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =     542.81 ms /   245 tokens (    2.22 ms per token,   451.35 tokens per second)\n",
            "llama_print_timings:        eval time =     311.29 ms /     5 runs   (   62.26 ms per token,    16.06 tokens per second)\n",
            "llama_print_timings:       total time =     863.39 ms /   250 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       7.74 ms /    13 runs   (    0.59 ms per token,  1680.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =     715.67 ms /   330 tokens (    2.17 ms per token,   461.10 tokens per second)\n",
            "llama_print_timings:        eval time =     786.76 ms /    12 runs   (   65.56 ms per token,    15.25 tokens per second)\n",
            "llama_print_timings:       total time =    1522.02 ms /   342 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       7.92 ms /    13 runs   (    0.61 ms per token,  1641.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =     695.51 ms /   266 tokens (    2.61 ms per token,   382.45 tokens per second)\n",
            "llama_print_timings:        eval time =     754.69 ms /    12 runs   (   62.89 ms per token,    15.90 tokens per second)\n",
            "llama_print_timings:       total time =    1470.64 ms /   278 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       3.79 ms /     6 runs   (    0.63 ms per token,  1582.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =     553.10 ms /   240 tokens (    2.30 ms per token,   433.92 tokens per second)\n",
            "llama_print_timings:        eval time =     312.34 ms /     5 runs   (   62.47 ms per token,    16.01 tokens per second)\n",
            "llama_print_timings:       total time =     874.25 ms /   245 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       3.52 ms /     6 runs   (    0.59 ms per token,  1704.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =     712.50 ms /   281 tokens (    2.54 ms per token,   394.39 tokens per second)\n",
            "llama_print_timings:        eval time =     321.68 ms /     5 runs   (   64.34 ms per token,    15.54 tokens per second)\n",
            "llama_print_timings:       total time =    1043.88 ms /   286 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       3.43 ms /     6 runs   (    0.57 ms per token,  1750.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =     536.52 ms /   241 tokens (    2.23 ms per token,   449.19 tokens per second)\n",
            "llama_print_timings:        eval time =     330.26 ms /     5 runs   (   66.05 ms per token,    15.14 tokens per second)\n",
            "llama_print_timings:       total time =     875.92 ms /   246 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       4.83 ms /     6 runs   (    0.81 ms per token,  1241.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =     699.87 ms /   282 tokens (    2.48 ms per token,   402.93 tokens per second)\n",
            "llama_print_timings:        eval time =     336.82 ms /     5 runs   (   67.36 ms per token,    14.84 tokens per second)\n",
            "llama_print_timings:       total time =    1048.20 ms /   287 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       4.80 ms /     6 runs   (    0.80 ms per token,  1248.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =     500.51 ms /   192 tokens (    2.61 ms per token,   383.61 tokens per second)\n",
            "llama_print_timings:        eval time =     336.98 ms /     5 runs   (   67.40 ms per token,    14.84 tokens per second)\n",
            "llama_print_timings:       total time =     851.03 ms /   197 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       9.79 ms /    12 runs   (    0.82 ms per token,  1225.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =     705.75 ms /   277 tokens (    2.55 ms per token,   392.49 tokens per second)\n",
            "llama_print_timings:        eval time =     732.39 ms /    11 runs   (   66.58 ms per token,    15.02 tokens per second)\n",
            "llama_print_timings:       total time =    1461.39 ms /   288 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       4.21 ms /     6 runs   (    0.70 ms per token,  1425.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =     548.32 ms /   213 tokens (    2.57 ms per token,   388.46 tokens per second)\n",
            "llama_print_timings:        eval time =     327.93 ms /     5 runs   (   65.59 ms per token,    15.25 tokens per second)\n",
            "llama_print_timings:       total time =     886.12 ms /   218 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       3.48 ms /     6 runs   (    0.58 ms per token,  1722.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =     510.52 ms /   187 tokens (    2.73 ms per token,   366.29 tokens per second)\n",
            "llama_print_timings:        eval time =     351.92 ms /     5 runs   (   70.38 ms per token,    14.21 tokens per second)\n",
            "llama_print_timings:       total time =     871.22 ms /   192 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       7.44 ms /    12 runs   (    0.62 ms per token,  1613.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =     530.33 ms /   228 tokens (    2.33 ms per token,   429.92 tokens per second)\n",
            "llama_print_timings:        eval time =     764.22 ms /    11 runs   (   69.47 ms per token,    14.39 tokens per second)\n",
            "llama_print_timings:       total time =    1312.77 ms /   239 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       3.50 ms /     6 runs   (    0.58 ms per token,  1714.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =     490.23 ms /   140 tokens (    3.50 ms per token,   285.58 tokens per second)\n",
            "llama_print_timings:        eval time =     343.91 ms /     5 runs   (   68.78 ms per token,    14.54 tokens per second)\n",
            "llama_print_timings:       total time =     842.58 ms /   145 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       7.37 ms /    12 runs   (    0.61 ms per token,  1628.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =     538.16 ms /   225 tokens (    2.39 ms per token,   418.09 tokens per second)\n",
            "llama_print_timings:        eval time =     757.40 ms /    11 runs   (   68.85 ms per token,    14.52 tokens per second)\n",
            "llama_print_timings:       total time =    1313.26 ms /   236 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       5.07 ms /     8 runs   (    0.63 ms per token,  1579.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =     521.03 ms /   161 tokens (    3.24 ms per token,   309.01 tokens per second)\n",
            "llama_print_timings:        eval time =     483.85 ms /     7 runs   (   69.12 ms per token,    14.47 tokens per second)\n",
            "llama_print_timings:       total time =    1016.97 ms /   168 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       3.52 ms /     6 runs   (    0.59 ms per token,  1702.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =     482.55 ms /   135 tokens (    3.57 ms per token,   279.76 tokens per second)\n",
            "llama_print_timings:        eval time =     353.25 ms /     5 runs   (   70.65 ms per token,    14.15 tokens per second)\n",
            "llama_print_timings:       total time =     844.55 ms /   140 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       6.67 ms /    11 runs   (    0.61 ms per token,  1648.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =     522.40 ms /   176 tokens (    2.97 ms per token,   336.91 tokens per second)\n",
            "llama_print_timings:        eval time =     704.00 ms /    10 runs   (   70.40 ms per token,    14.20 tokens per second)\n",
            "llama_print_timings:       total time =    1243.24 ms /   186 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       4.24 ms /     6 runs   (    0.71 ms per token,  1416.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =     579.88 ms /   245 tokens (    2.37 ms per token,   422.50 tokens per second)\n",
            "llama_print_timings:        eval time =     330.74 ms /     5 runs   (   66.15 ms per token,    15.12 tokens per second)\n",
            "llama_print_timings:       total time =     927.58 ms /   250 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       8.82 ms /    13 runs   (    0.68 ms per token,  1474.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =     762.69 ms /   330 tokens (    2.31 ms per token,   432.68 tokens per second)\n",
            "llama_print_timings:        eval time =     836.90 ms /    12 runs   (   69.74 ms per token,    14.34 tokens per second)\n",
            "llama_print_timings:       total time =    1621.81 ms /   342 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       9.06 ms /    12 runs   (    0.76 ms per token,  1324.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =     725.04 ms /   266 tokens (    2.73 ms per token,   366.87 tokens per second)\n",
            "llama_print_timings:        eval time =     781.46 ms /    11 runs   (   71.04 ms per token,    14.08 tokens per second)\n",
            "llama_print_timings:       total time =    1529.68 ms /   277 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       4.47 ms /     6 runs   (    0.75 ms per token,  1341.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =     570.29 ms /   240 tokens (    2.38 ms per token,   420.84 tokens per second)\n",
            "llama_print_timings:        eval time =     345.73 ms /     5 runs   (   69.15 ms per token,    14.46 tokens per second)\n",
            "llama_print_timings:       total time =     929.55 ms /   245 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       3.94 ms /     6 runs   (    0.66 ms per token,  1524.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =     759.10 ms /   281 tokens (    2.70 ms per token,   370.18 tokens per second)\n",
            "llama_print_timings:        eval time =     359.61 ms /     5 runs   (   71.92 ms per token,    13.90 tokens per second)\n",
            "llama_print_timings:       total time =    1129.03 ms /   286 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       3.67 ms /     6 runs   (    0.61 ms per token,  1637.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =     568.24 ms /   240 tokens (    2.37 ms per token,   422.35 tokens per second)\n",
            "llama_print_timings:        eval time =     364.31 ms /     5 runs   (   72.86 ms per token,    13.72 tokens per second)\n",
            "llama_print_timings:       total time =     942.15 ms /   245 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       5.45 ms /     9 runs   (    0.61 ms per token,  1652.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =     745.17 ms /   281 tokens (    2.65 ms per token,   377.10 tokens per second)\n",
            "llama_print_timings:        eval time =     555.61 ms /     8 runs   (   69.45 ms per token,    14.40 tokens per second)\n",
            "llama_print_timings:       total time =    1315.88 ms /   289 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       3.48 ms /     6 runs   (    0.58 ms per token,  1725.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =     581.47 ms /   241 tokens (    2.41 ms per token,   414.47 tokens per second)\n",
            "llama_print_timings:        eval time =     354.01 ms /     5 runs   (   70.80 ms per token,    14.12 tokens per second)\n",
            "llama_print_timings:       total time =     944.54 ms /   246 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1328.97 ms\n",
            "llama_print_timings:      sample time =       3.65 ms /     6 runs   (    0.61 ms per token,  1645.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =     760.19 ms /   282 tokens (    2.70 ms per token,   370.96 tokens per second)\n",
            "llama_print_timings:        eval time =     343.12 ms /     5 runs   (   68.62 ms per token,    14.57 tokens per second)\n",
            "llama_print_timings:       total time =    1113.27 ms /   287 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: {0: {'v_reg': '0.845', 'v_oc': '2: moderately positive emotional state can be inferred', 'e_c': ['joy', 'optimism'], 'ei_reg': [{'joy': '0.654'}], 'ei_oc': [{'joy': ['2.46', 'Aberrant answer from EmoLLM : 2.46']}]}, 1: {'v_reg': '0.85', 'v_oc': '3: very positive emotional state can be inferred', 'e_c': ['joy', 'optimism'], 'ei_reg': [{'joy': '0.75'}], 'ei_oc': [{'joy': ['3: high amount of joy can be inferred', '3: high amount of joy can be inferred']}]}, 2: {'v_reg': '0.583', 'v_oc': '2: moderately positive emotional state can be inferred', 'e_c': ['anger', 'disgust', 'optimism', 'sadness'], 'ei_reg': [{'anger': '0.521'}, {'sadness': '0.345'}], 'ei_oc': [{'anger': ['1.458', 'Aberrant answer from EmoLLM : 1.458']}, {'sadness': ['0.346', 'Aberrant answer from EmoLLM : 0.346']}]}}, 1: {0: {'v_reg': '0.917', 'v_oc': '3: very positive emotional state can be inferred', 'e_c': ['joy', 'optimism'], 'ei_reg': [{'joy': '0.729'}], 'ei_oc': [{'joy': ['2: moderate amount of joy can be inferred', '2: moderate amount of joy can be inferred']}]}, 1: {'v_reg': '0.942', 'v_oc': '3: very positive emotional state can be inferred', 'e_c': ['joy', 'optimism', 'surprise'], 'ei_reg': [{'joy': '0.731'}], 'ei_oc': [{'joy': ['3: high amount of joy can be inferred', '3: high amount of joy can be inferred']}]}, 2: {'v_reg': '0.435', 'v_oc': '0: neutral or mixed emotional state can be inferred', 'e_c': ['anger', 'disgust', 'joy', 'sadness'], 'ei_reg': [{'anger': '0.426'}, {'joy': '0.167'}, {'sadness': '0.417'}], 'ei_oc': [{'anger': ['1.458', 'Aberrant answer from EmoLLM : 1.458']}, {'joy': ['0: no joy can be inferred', '0: no joy can be inferred']}, {'sadness': ['0.292', 'Aberrant answer from EmoLLM : 0.292']}]}}}\n"
          ]
        }
      ],
      "source": [
        "outputs = sentiment_analysis(sites_data, 500)\n",
        "print(outputs)\n",
        "with open('output_sentiment_analysis_data.json', 'w', encoding='utf-8') as fp:\n",
        "    json.dump(outputs, fp)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e3513fe77d494cd2bd9a47862a875e2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e04e446739e54243876cab925b47e3c6",
              "IPY_MODEL_a15ad7e46760494aa4a728823e1f1bbd",
              "IPY_MODEL_17a1955e745c47a99bdc3d191756823e"
            ],
            "layout": "IPY_MODEL_6c83076f584949cc9a6d20f5995e0bc9"
          }
        },
        "e04e446739e54243876cab925b47e3c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07db6f1b23f745c6b1586e711ac3ec6d",
            "placeholder": "​",
            "style": "IPY_MODEL_ba60b427fa9a4abf98f7760050903c5b",
            "value": "Emollama-chat-13b-v0.1.gguf: 100%"
          }
        },
        "a15ad7e46760494aa4a728823e1f1bbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd7078604c8a4f45b721df7f1743019e",
            "max": 13831319488,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_961b41a60c7c41aa96ce41d60fe3ef6e",
            "value": 13831319488
          }
        },
        "17a1955e745c47a99bdc3d191756823e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79c4ce03bc4e49609704522c54ec559e",
            "placeholder": "​",
            "style": "IPY_MODEL_8df68635a3ba462d98affdee698128bd",
            "value": " 13.8G/13.8G [41:07&lt;00:00, 3.12MB/s]"
          }
        },
        "6c83076f584949cc9a6d20f5995e0bc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07db6f1b23f745c6b1586e711ac3ec6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba60b427fa9a4abf98f7760050903c5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd7078604c8a4f45b721df7f1743019e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "961b41a60c7c41aa96ce41d60fe3ef6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79c4ce03bc4e49609704522c54ec559e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8df68635a3ba462d98affdee698128bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}