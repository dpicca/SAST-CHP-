{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv6gal8sMv6L"
      },
      "source": [
        "## MODEL SETUP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-Xg97HdJNOg"
      },
      "source": [
        "Installing [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) using CMake.\n",
        "This might take some time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPsnG7iZFBRl",
        "outputId": "9eb88892-7e75-4ea7-fb6a-6e2055d19bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.75.tar.gz (48.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.75-cp310-cp310-linux_x86_64.whl size=76309122 sha256=9a1b9ea2ff24cb49d36f369eac77898332c8d3d435df9a7b834bb5f4c7d6bc74\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/df/9a/e4bb2e48bfa64fb174f0f786296c8507dbebea2a112f1adf8d\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.75\n"
          ]
        }
      ],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbudugHaLOO1"
      },
      "source": [
        "Importing what is needed for training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74mM_eUqFgmc"
      },
      "outputs": [],
      "source": [
        "# Allows us to go grab models from hugging face\n",
        "from huggingface_hub import hf_hub_download\n",
        "# The Llama class is a wrapper for llama cpp models\n",
        "from llama_cpp import Llama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxa5RKHgK1uL"
      },
      "source": [
        "This bit of code allows us to go grabe the model from Hugging Face.\n",
        "This might take some time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXZ3uHjIFr0F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207,
          "referenced_widgets": [
            "7e9f9e2ca5c340dc8f614653abe834f9",
            "5bef624aabd34052ba2cc78598d93564",
            "1943e226f2e44eb09605765c8620da33",
            "85578d154aca42a3814c8a2b6eb57e1f",
            "5eccc971e9924e90bc46b92af3100a3e",
            "c62ecbb4e5e54299a4a960fa2e67ec5f",
            "6df12e92732142a8b998da5f06f7fe0b",
            "91a2caac8c9849088ecfb291664a007a",
            "acfa785a61964634837228474fdf89bd",
            "f92cff3089d142c586ffc2a6b355d223",
            "2db422076c294c25bd5e743d4e683818"
          ]
        },
        "outputId": "bc8a0ad7-7414-4d72-af8d-872b93e3cbbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Emollama-chat-13b-v0.1.gguf:   0%|          | 0.00/13.8G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e9f9e2ca5c340dc8f614653abe834f9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_name = \"myclassunil/Emollama-chat-13b-v0.1.gguf\"\n",
        "model_file = \"Emollama-chat-13b-v0.1.gguf\"\n",
        "model_path = hf_hub_download(model_name,\n",
        "                             filename=model_file,\n",
        "                             local_dir='/content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yEanZvZMZdG"
      },
      "source": [
        "Loading the model and offloading all the layers to the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-4aAjGXGBd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98063b1a-703f-4beb-83ec-8f2675b98580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 21 key-value pairs and 363 tensors from /content/Emollama-chat-13b-v0.1.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = .\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 40\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type q8_0:  282 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 5120\n",
            "llm_load_print_meta: n_head           = 40\n",
            "llm_load_print_meta: n_head_kv        = 40\n",
            "llm_load_print_meta: n_layer          = 40\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
            "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 13824\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 13B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 13.02 B\n",
            "llm_load_print_meta: model size       = 12.88 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = .\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.37 MiB\n",
            "llm_load_tensors: offloading 40 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 41/41 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   166.02 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size = 13023.85 MiB\n",
            "....................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   400.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =    85.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    11.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1286\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.model': 'llama', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '2048', 'general.name': '.', 'llama.vocab_size': '32000', 'general.file_type': '7', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '40', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ],
      "source": [
        "llm = Llama(model_path=model_path,\n",
        "            n_gpu_layers=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BASkMhLQMtk-"
      },
      "source": [
        "The model is now usable!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHUD0S8nNVdh"
      },
      "source": [
        "## DATA SETUP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NlxU4zcNdPa"
      },
      "source": [
        "Grabbing the reviews from our json file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wfQv1nL4NpyN"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "filename = 'json_reviews_examples.json'\n",
        "path_to_data = str(filename)\n",
        "\n",
        "with open(path_to_data) as f:\n",
        "    sites = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQFdj2J-3d6A"
      },
      "source": [
        "Making sure that everything works properly by printing the reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikS0RTqi3jfK",
        "outputId": "aa4372a4-6bca-481b-ab07-44cfa0ad40a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This place is well worth the visit and make sure to book your all inclusive ticket well in advance, which will let you skip with waiting lines, and will also give you access to the Roman Forum.\n",
            "I would definitely allow yourself at least a couple of hours to walk around the Forum as it is such a vast space with lots to see and do. Your all access tickets will allow you to see the exhibitions inside too.\n",
            "As for the Colosseum, what an incredible piece of history. Words cannot describe how stunning this structure is. Definitely a highlight of our trip in Rome.\n",
            "\n",
            "As a lover of history, I can't recommend visiting enough. It's absolutely breathtaking. Pictures can't do justice to the scale and beauty of this monument. Highly recommend booking a tour in advance to skip the queues and to learn so much more about the place. It is completely worth doing the underground also.\n",
            "\n",
            "The restoration and landscaping of the Colosseum have been done very well. It manages to shine from the moment you see it until the last glance you take as you leave. I recommend buying tickets online due to the crowds. Be sure to pay attention to the timing when selecting your online tickets and be there at the specified time, or else you won't be able to enter.\n",
            "The organization of the entrance area was very poor. Unfortunately, I find the ticket prices to be quite expensive, and I don't think the regular ticket meets expectations. We bought tickets that also provided access to the Arena, Underground, and III Level of the Colosseum with a guide. It was really expensive, and although we were generally satisfied with our visit, I don't think it was worth the money we spent.\n",
            "\n",
            "This place is well worth the visit and make sure to book your all inclusive ticket well in advance, which will let you skip with waiting lines, and will also give you access to the Roman Forum.\n",
            "I would definitely allow yourself at least a couple of hours to walk around the Forum as it is such a vast space with lots to see and do. Your all access tickets will allow you to see the exhibitions inside too.\n",
            "As for the Colosseum, what an incredible piece of history. Words cannot describe how stunning this structure is. Definitely a highlight of our trip in Rome.\n",
            "\n",
            "As a lover of history, I can't recommend visiting enough. It's absolutely breathtaking. Pictures can't do justice to the scale and beauty of this monument. Highly recommend booking a tour in advance to skip the queues and to learn so much more about the place. It is completely worth doing the underground also.\n",
            "\n",
            "The restoration and landscaping of the Colosseum have been done very well. It manages to shine from the moment you see it until the last glance you take as you leave. I recommend buying tickets online due to the crowds. Be sure to pay attention to the timing when selecting your online tickets and be there at the specified time, or else you won't be able to enter.\n",
            "The organization of the entrance area was very poor. Unfortunately, I find the ticket prices to be quite expensive, and I don't think the regular ticket meets expectations. We bought tickets that also provided access to the Arena, Underground, and III Level of the Colosseum with a guide. It was really expensive, and although we were generally satisfied with our visit, I don't think it was worth the money we spent.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for site in sites:\n",
        "  for review in sites[site]:\n",
        "    print(review + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyJYrJTbM3EF"
      },
      "source": [
        "## USING EMOLLM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing reegular expressions for later use."
      ],
      "metadata": {
        "id": "N27EAdTUxAbc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ss8juweTPS5a"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yp7AQTw3opS"
      },
      "source": [
        "We'll start by defining a function for each one of the five tasks accessible through EmoLLM.\n",
        "For transparency reasons, the names of the variables are taken from [\"EmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective Analysis\"](https://doi.org/10.48550/arXiv.2401.08508) (Zhiwei Liu and others, 2024)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F06ovpdz3x6m"
      },
      "outputs": [],
      "source": [
        "# We need to define the values we'll deem acceptable from the LLM.\n",
        "\n",
        "# Possible answers for the v_oc task.\n",
        "acceptable_v_oc = ['3: very positive mental state can be inferred',\n",
        "                   '2: moderately positive mental state can be inferred',\n",
        "                   '1: slightly positive mental state can be inferred',\n",
        "                   '0: neutral or mixed mental state can be inferred',\n",
        "                   '-1: slightly negative mental state can be inferred',\n",
        "                   '-2: moderately negative mental state can be inferred',\n",
        "                   '-3: very negative mental state can be inferred']\n",
        "\n",
        "# Possible answers for the e_c task.\n",
        "acceptable_e_c = ['anger',\n",
        "                  'anticipation',\n",
        "                  'disgust',\n",
        "                  'fear',\n",
        "                  'joy',\n",
        "                  'love',\n",
        "                  'optimism',\n",
        "                  'pessimism',\n",
        "                  'sadness',\n",
        "                  'surprise',\n",
        "                  'trust']\n",
        "\n",
        "# This is the list of sentiments that are supported for ei_reg and ei_oc.\n",
        "main_sentiments = ['joy', 'anger', 'fear', 'sadness']\n",
        "\n",
        "# Possible answers for the ei_oc task.\n",
        "base_ei_oc = ['0: no E can be inferred',\n",
        "              '1: low amount of E can be inferred',\n",
        "              '2: moderate amount of E can be inferred',\n",
        "              '3: high amount of E can be inferred']\n",
        "\n",
        "# Adapting the possible answers to include particular emotions.\n",
        "acceptable_ei_oc = []\n",
        "for sentiment in main_sentiments:\n",
        "  for message in base_ei_oc:\n",
        "    acceptable_ei_oc.append(message.replace(\"E\", sentiment))\n",
        "\n",
        "# Estimating the valence of the review and representing it as a float.\n",
        "def get_v_reg(text, max_tokens):\n",
        "  prompt= f'''\n",
        "  Human:\n",
        "  Task: Evaluate the valence intensity of the writer's mental state based on the text, assigning it a real-valued score from 0 (most negative) to 1 (most positive).\n",
        "  Text: {text}\n",
        "  Intensity Score:\n",
        "  '''\n",
        "  v_reg = \"Aberrant answer from EmoLLM : \"\n",
        "  raw_v_reg = llm(prompt, max_tokens=max_tokens)['choices'][0]['text']\n",
        "  try:\n",
        "    if(0 <= float(raw_v_reg) <= 1):\n",
        "      v_reg = raw_v_reg\n",
        "  except:\n",
        "    print(\"Something went wrong with get_v_reg\")\n",
        "    v_reg += str(raw_v_reg)\n",
        "  return v_reg\n",
        "\n",
        "# For each task, the default value is \"Aberrant answer from EmoLLM\"\n",
        "# if the model does well, we replace this message with its answer\n",
        "# otherwise, we add the aberrant answer in case it holds useful data anyway.\n",
        "\n",
        "# Estimates the valence of the review and represents it as an ordinal class.\n",
        "def get_v_oc(text, max_tokens):\n",
        "  prompt = f'''\n",
        "  Human:\n",
        "  Task: Categorize the text into an ordinal class that best characterizes the writer's mental state, considering various degrees of positive and negative sentiment intensity. 3: very positive mental state can be inferred. 2: moderately positive mental state can be inferred. 1: slightly positive mental state can be inferred. 0: neutral or mixed mental state can be inferred. -1: slightly negative mental state can be inferred. -2: moderately negative mental state can be inferred. -3: very negative mental state can be inferred\n",
        "  Text: {text}\n",
        "  Intensity Class:\n",
        "  '''\n",
        "  v_oc = \"Aberrant answer from EmoLLM : \"\n",
        "  raw_v_oc = llm(prompt, max_tokens=max_tokens)['choices'][0]['text']\n",
        "  if(raw_v_oc in acceptable_v_oc):\n",
        "    v_oc = raw_v_oc\n",
        "  else:\n",
        "    v_oc += str(raw_v_oc)\n",
        "  return raw_v_oc\n",
        "\n",
        "# Identifies which sentiments are present in the review.\n",
        "def get_e_c(text, max_tokens):\n",
        "  prompt = f'''\n",
        "  Task: Categorize the text's emotional tone as either 'neutral or no emotion' or identify the presence of one or more of the given emotions (anger, anticipation, disgust, fear, joy, love, optimism, pessimism, sadness, surprise, trust).\n",
        "  Text: {text}\n",
        "  This tweet contains emotions:\n",
        "  '''\n",
        "  e_c = \"Aberrant answer from EmoLLM : \"\n",
        "  raw_e_c = llm(prompt, max_tokens=max_tokens)['choices'][0]['text']\n",
        "  temp_e_c = re.split(r'[^a-zA-Z]', raw_e_c)\n",
        "  while('' in temp_e_c):\n",
        "    temp_e_c.remove('')\n",
        "  is_aberrant = False\n",
        "  for s in temp_e_c:\n",
        "    if(s not in acceptable_e_c):\n",
        "      is_aberrant = True\n",
        "      break\n",
        "  if(is_aberrant == False):\n",
        "    e_c = temp_e_c\n",
        "  else:\n",
        "    e_c += str(raw_e_c)\n",
        "  return e_c\n",
        "\n",
        "# Estimates the intensity of a sentiment and represents it as a float.\n",
        "def get_ei_reg(text, max_tokens, sentiment):\n",
        "  prompt = f'''\n",
        "    Human:\n",
        "    Task: Assign a numerical value between 0 (least E) and 1 (most E) to represent the intensity of emotion E expressed in the text.\n",
        "    Text: {text}\n",
        "    Emotion: {sentiment}\n",
        "    Intensity Score:\n",
        "  '''\n",
        "  ei_reg = \"Aberrant answer from EmoLLM : \"\n",
        "  raw_ei_reg = llm(prompt, max_tokens=max_tokens)['choices'][0]['text']\n",
        "  try:\n",
        "    if(0 <= float(raw_ei_reg) <= 1):\n",
        "      ei_reg = raw_ei_reg\n",
        "  except:\n",
        "    print(\"Something went wrong with get_ei_reg\")\n",
        "    ei_reg += str(raw_ei_reg)\n",
        "  return ei_reg\n",
        "\n",
        "# Estimates the intensity of a sentiment and represents it as an ordinal class.\n",
        "def get_ei_oc(text, max_tokens, sentiment):\n",
        "  prompt = f'''\n",
        "  Task: Categorize the tweet into an intensity level of the specified emotion E, representing the mental state of the tweeter. 0: no E can be inferred. 1: low amount of E can be inferred. 2: moderate amount of E can be inferred. 3: high amount of E can be inferred.\n",
        "  Tweet: {text}\n",
        "  Emotion: {sentiment}\n",
        "  Intensity Score:\n",
        "  '''\n",
        "  ei_oc = \"Aberrant answer from EmoLLM : \"\n",
        "  raw_ei_oc = llm(prompt, max_tokens=max_tokens)['choices'][0]['text']\n",
        "  if(raw_ei_oc in acceptable_ei_oc):\n",
        "    ai_oc = raw_ei_oc\n",
        "  else:\n",
        "    ai_oc += str(raw_ei_oc)\n",
        "  return [raw_ei_oc, ei_oc]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now create a function that applies the five tools given by EmoLLM to our reviews."
      ],
      "metadata": {
        "id": "XCu_dc6qx11u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nOPJD6ZbC67c"
      },
      "outputs": [],
      "source": [
        "# Processing the reviews using EmoLLM\n",
        "def sentiment_analysis(sites,max_tokens):\n",
        "  # We create a dict to hold the sentiment analysis results.\n",
        "  sa_results = {}\n",
        "  # We create a dict for each site results.\n",
        "  for i, site in enumerate(sites):\n",
        "    sa_site = {}\n",
        "    # Processing the different reviews with the five EmoLLM tasks.\n",
        "    for j, review in enumerate(sites[site]):\n",
        "      v_reg = get_v_reg(review, max_tokens)\n",
        "      v_oc = get_v_oc(review, max_tokens)\n",
        "      e_c = get_e_c(review, max_tokens)\n",
        "      ei_reg = []\n",
        "      ei_oc = []\n",
        "      for sentiment in e_c:\n",
        "        if(sentiment in main_sentiments):\n",
        "          current_reg = {sentiment : get_ei_reg(review, max_tokens, sentiment)}\n",
        "          current_oc = {sentiment : get_ei_oc(review, max_tokens, sentiment)}\n",
        "          ei_reg.append(current_reg)\n",
        "          ei_oc.append(current_oc)\n",
        "      sa_review = {\n",
        "        'v_reg' : v_reg,\n",
        "        'v_oc' : v_oc,\n",
        "        'e_c' : e_c,\n",
        "        'ei_reg' : ei_reg,\n",
        "        'ei_oc' : ei_oc\n",
        "      }\n",
        "      sa_site[j] = sa_review\n",
        "    sa_results[i] = sa_site\n",
        "  return sa_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we have limited ressources for this project, the possibility to use GPU was limited. We needed to generate placeholder results. This function is useless for the final state of the project. It generates a mockup version of the output file.\n",
        "This function is not perfect. For example, regressions and ordinal classifications are not corellated. Furthermore, it does not generate aberrant answers."
      ],
      "metadata": {
        "id": "UEgkwXDYybVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The random library is needed to generate random data\n",
        "import random\n",
        "# As GPU ressource are scarce, this function creates a placeholder for data.\n",
        "def mockup_sentiment_analysis(sites):\n",
        "  # We create a dict to hold the sentiment analysis results.\n",
        "  sa_results = {}\n",
        "  # We create a dict for each site results.\n",
        "  for i, site in enumerate(sites):\n",
        "    sa_site = {}\n",
        "    # Processing the different reviews with the five EmoLLM tasks.\n",
        "    for j, review in enumerate(sites[site]):\n",
        "      v_reg = str(round(random.random(), 3))\n",
        "      v_oc = random.choice(acceptable_v_oc)\n",
        "      e_c = []\n",
        "      for r in range(random.randrange(1, len(acceptable_e_c))):\n",
        "        random_e_c = random.choice(acceptable_e_c)\n",
        "        if(random_e_c not in e_c):\n",
        "          e_c.append(random_e_c)\n",
        "      ei_reg = []\n",
        "      ei_oc = []\n",
        "      for sentiment in e_c:\n",
        "        if(sentiment in main_sentiments):\n",
        "          current_reg = {sentiment : str(round(random.random(), 3))}\n",
        "          random_oc = random.choice(base_ei_oc).replace(\"E\", sentiment)\n",
        "          current_oc = {sentiment : random_oc}\n",
        "          ei_reg.append(current_reg)\n",
        "          ei_oc.append(current_oc)\n",
        "      sa_review = {\n",
        "        'v_reg' : v_reg,\n",
        "        'v_oc' : v_oc,\n",
        "        'e_c' : e_c,\n",
        "        'ei_reg' : ei_reg,\n",
        "        'ei_oc' : ei_oc\n",
        "      }\n",
        "      sa_site[f'review_{j}'] = sa_review\n",
        "    sa_results[site] = sa_site\n",
        "  return sa_results"
      ],
      "metadata": {
        "id": "huJyQ89Yn296"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can dump the output of this process in a json file."
      ],
      "metadata": {
        "id": "hq9AT3Y5zB3F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MI4YBWCa9OiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32209a7a-a760-4999-9e0c-b0becdb527ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Tour Eiffel': {'review_0': {'v_reg': '0.229', 'v_oc': '3: very positive mental state can be inferred', 'e_c': ['surprise', 'joy', 'pessimism', 'disgust', 'fear', 'sadness', 'anger'], 'ei_reg': [{'joy': '0.147'}, {'fear': '0.049'}, {'sadness': '0.855'}, {'anger': '0.478'}], 'ei_oc': [{'joy': '0: no joy can be inferred'}, {'fear': '3: high amount of fear can be inferred'}, {'sadness': '0: no sadness can be inferred'}, {'anger': '2: moderate amount of anger can be inferred'}]}, 'review_1': {'v_reg': '0.833', 'v_oc': '0: neutral or mixed mental state can be inferred', 'e_c': ['love', 'sadness', 'joy'], 'ei_reg': [{'sadness': '0.961'}, {'joy': '0.354'}], 'ei_oc': [{'sadness': '3: high amount of sadness can be inferred'}, {'joy': '3: high amount of joy can be inferred'}]}, 'review_2': {'v_reg': '0.444', 'v_oc': '2: moderately positive mental state can be inferred', 'e_c': ['disgust'], 'ei_reg': [], 'ei_oc': []}}, 'Colisee': {'review_0': {'v_reg': '0.975', 'v_oc': '-1: slightly negative mental state can be inferred', 'e_c': ['pessimism'], 'ei_reg': [], 'ei_oc': []}, 'review_1': {'v_reg': '0.48', 'v_oc': '-1: slightly negative mental state can be inferred', 'e_c': ['pessimism', 'love', 'surprise', 'anticipation', 'trust', 'sadness', 'optimism'], 'ei_reg': [{'sadness': '0.198'}], 'ei_oc': [{'sadness': '0: no sadness can be inferred'}]}, 'review_2': {'v_reg': '0.066', 'v_oc': '1: slightly positive mental state can be inferred', 'e_c': ['anger', 'pessimism', 'sadness', 'love', 'trust'], 'ei_reg': [{'anger': '0.839'}, {'sadness': '0.736'}], 'ei_oc': [{'anger': '2: moderate amount of anger can be inferred'}, {'sadness': '1: low amount of sadness can be inferred'}]}}}\n"
          ]
        }
      ],
      "source": [
        "placeholder = mockup_sentiment_analysis(sites)\n",
        "print(placeholder)\n",
        "with open('placeholder_sentiment_analysis_data.json', 'w') as fp:\n",
        "    json.dump(placeholder, fp)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "Cv6gal8sMv6L"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7e9f9e2ca5c340dc8f614653abe834f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5bef624aabd34052ba2cc78598d93564",
              "IPY_MODEL_1943e226f2e44eb09605765c8620da33",
              "IPY_MODEL_85578d154aca42a3814c8a2b6eb57e1f"
            ],
            "layout": "IPY_MODEL_5eccc971e9924e90bc46b92af3100a3e"
          }
        },
        "5bef624aabd34052ba2cc78598d93564": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c62ecbb4e5e54299a4a960fa2e67ec5f",
            "placeholder": "​",
            "style": "IPY_MODEL_6df12e92732142a8b998da5f06f7fe0b",
            "value": "Emollama-chat-13b-v0.1.gguf: 100%"
          }
        },
        "1943e226f2e44eb09605765c8620da33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91a2caac8c9849088ecfb291664a007a",
            "max": 13831319488,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_acfa785a61964634837228474fdf89bd",
            "value": 13831319488
          }
        },
        "85578d154aca42a3814c8a2b6eb57e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f92cff3089d142c586ffc2a6b355d223",
            "placeholder": "​",
            "style": "IPY_MODEL_2db422076c294c25bd5e743d4e683818",
            "value": " 13.8G/13.8G [05:40&lt;00:00, 80.8MB/s]"
          }
        },
        "5eccc971e9924e90bc46b92af3100a3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c62ecbb4e5e54299a4a960fa2e67ec5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6df12e92732142a8b998da5f06f7fe0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91a2caac8c9849088ecfb291664a007a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acfa785a61964634837228474fdf89bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f92cff3089d142c586ffc2a6b355d223": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2db422076c294c25bd5e743d4e683818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
